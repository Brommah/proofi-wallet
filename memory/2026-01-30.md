# 2026-01-30

## Clay.com - Lead Scoring for Blockchain Engineer Candidates

### Context
Mart is working in Clay.com (app.clay.com) on a "Find People" table with 128 blockchain engineer candidates for Cerebellum Network (Cere). The workspace is "Team Growth - Cerebellum Network Inc".

### What we did
- Opened Clay's "Use AI" panel on the Find people Table
- Changed the Use case from **"Web research (Claygent)"** (3 credits/row) to **"Create or modify content"** (cheaper, no internet needed since data is already enriched)
- Model: **OpenAI > GPT 4o** with **OpenAI API Key Cere**
- Available columns: Find people, Name, Title, Org, Headline, Summary, Location, LinkedIn Profile, plus enrichment columns

### Scoring prompt we wanted to use
Score blockchain engineer candidates 1-10 for an AI+blockchain infrastructure company. Criteria:
- Protocol-level depth, Rust/Substrate experience, production blockchain work
- dApp-only or no Rust = max 4
- No protocol work = max 5
- Score conservatively

Input columns: Title, Org, Headline, Summary, Location

### Blocker
- Clay uses a custom text editor (not standard textarea/contenteditable) for the prompt field
- Browser automation couldn't find or fill the prompt textarea
- Session ended while trying to solve this

### Next steps
- Either manually paste the prompt into Clay, or find the correct DOM element for the prompt editor
- After scoring: filter top candidates, potentially export for outreach
- The cere-hr-service project also exists at `/Users/martijnbroersma/clawd/cere-hr-service/` for HR automation

## Lemlist API Import - SUCCESS
- **API Key:** stored (f05af...920b)
- **Campaign:** "Blockchain Engineers - Clay" (cam_39ENSf8fmJH3ikQM2)
- **Imported:** 190/197 leads from Clay CSV via Lemlist API
- **Method:** Generated emails from firstname.lastname@companyDomain, POST to /api/campaigns/{id}/leads/{email}
- **Errors:** 7 (3 test dupes, 4 invalid email formatting)
- **Next:** Set up email sequences for outreach

## Cere HR Service
- Project at `/Users/martijnbroersma/clawd/cere-hr-service/`
- Has OpenAI integration (`src/services/openai.ts`)
- README exists with project documentation

---

## Call with Fred & Val - Transcript Summary (Jan 30)

### Context
Team Charlie (hiring/growth team) weekly review. Fred (CTO) + Val on the call. Fred opened with feedback on the week's execution cadence and ownership, then Mart walked through the hiring pipeline and automation work.

### 1. Key Discussion Points

#### A. Extreme Ownership & Daily Check-ins
Fred's core message: **Team Charlie lacks ownership cadence.** He wants daily check-ins (Tue/Wed mid-week updates), not waiting until Friday to report. 
> **Fred:** *"All we gotta do is just get into this mode of extreme ownership. That's all. Accountability discipline is good for all of us."*
> **Fred:** *"Team Charlie is in the state where it is because no one has really taken full ownership over it."*
> **Fred:** *"My main gripe... it's just simply we don't get to have these conversations, or they don't happen early enough."*

**Takeaway:** Proactively flag problems mid-week. Don't wait. Even "we screwed up, here's the fix" is better than silence until Friday.

#### B. AI Scoring & Automation Pipeline (Mart's Demo)
Mart walked Fred through the full hiring automation:
- **Pipeline cleanup:** Cleaned the HR board â€” removed hundreds of miscategorized candidates skewing stats. Now single clean view.
- **AI Scoring:** Migrated from GPT-4 â†’ **Gemini Flash 3** (free on workspace package). More structured evaluation output. **MAE (Mean Absolute Error) between human and AI scores has drastically dropped.**
- **Version tracking:** Job role definitions synced with GitHub + Notion. When prompts change, new versions are generated and efficacy is measured between snapshots.
- **Daily eval tracker:** Takes last 10 evaluations daily, checks if scores are within acceptable range, alerts when drift occurs.
- **Clawdbot integration:** Can ask Clawdbot to review last 20 applications and check MAE against updated prompts.

Fred acknowledged: *"There's a lot of good stuff here."* But wants it **bucketed and presentable** â€” not all unpacked at once.

#### C. Source Tracking & Inbound Automation
- **Source attribution dashboard** now complete â€” can distinguish Join.com vs WellFound vs SiriNet vs CEF website origins
- **Kicked out Zapier** â€” now pulling Join.com directly to fetch resumes, running through the same scoring pipeline
- **WellFound:** Google Apps Script polls inbox every 50 minutes, extracts resumes to Google Drive, parses and runs through system
- **Career pages:** All updated (were broken/wrong roles before). Lever directs fixed.
- **X Ads campaign:** Deployed for Innovation Manager role. Quick tests for Blockchain Engineer starting.

#### D. Outbound / Clay Sourcing
- Using Clawdbot to generate custom candidate lists via natural language
- Sourcing strategy documented in Google Doc: GitHub repos, open source contributors, distributed systems talent
- Auto-proposes email sequences based on job description + criteria â†’ push to Lemlist
- Everything documented in **Notion Team Growth model**

#### E. Standardization & Team AI Tooling
Fred pushed hard on standardization across the team:
> **Fred:** *"There's no standardization. Five, six different IDEs and models the engineering team uses. Unless we all get together and do these very intentional workshops, we're not gonna standardize."*

Topics: shared AI models, local models (DeepSeek, Qwen, open source on Mac Minis), common evals, shared best practices, cost control on AI tool subscriptions.

Mart showed his **"Phased approach to unify AI developments"** doc â€” inventory of who uses what tools/roles. Fred liked it but wants it cross-referenced with what engineering (Ehor, Sergei) is doing.

#### F. Hackathon Prep (3 weeks out, in Salo)
Fred is putting heavy emphasis on preparation:
> **Fred:** *"We have three weeks of everyone getting together, and this is where it'll be tremendously beneficial for anyone who comes in focused."*

**Hackathon structure = "The What" + "The How":**
- **The What:** Specific, modular, pointed topics needed for product/demos + internal process improvements
- **The How:** Best practices for AI design, eval, testing. Shared toolkit.

Fred wants Mart to prepare:
1. List of hackathon topics (system improvements, hiring automation features)
2. Toolkit he's bringing (tools, methods, learnings)
3. Compare with what engineering is doing
4. Show leadership by saying "This is what I'm bringing to the table" â†’ force others to do the same

**Logistics:** Only 7 bedrooms in Salo. Mart may need to bunk up or find own accommodation.

#### G. Interview Scripts (This Week's Deliverable)
Two scripts needed:
1. **15-minute pre-interview script** â€” for Val/Mart to pre-screen candidates
2. **Longer engineering interview script** â€” for Patch/Sergey/Fred to use, standardized structure with 2-3 critical questions

Fred shared an article/link as starting point. He edited it and will push to Mart + Val for review.

**Candidate routing logic:**
- Rock stars (flagged by AI) â†’ fast-track through full process
- Above-threshold score â†’ 15-minute pre-interview
- Overdue candidates (2+ days) â†’ automated alert/notification

#### H. Dashboard Vision
Fred wants a dashboard that shows:
- **Volume** (inbound/outbound by source, day-by-day)
- **Quality overlay** (scoring distribution)
- **Real metric:** "Rock star rate" â€” how many potential top-tier candidates per day/week
> **Fred:** *"The real measurement is, do we get one rock star, two rock stars... What's the rate for us to find these rock stars?"*

Target: not the mean candidate, but **mean + couple standard deviations up** (top 10%).

### 2. Action Items for Mart

| # | Action | Deadline | Priority |
|---|--------|----------|----------|
| 1 | **Daily check-ins** â€” Post execution updates daily (not just Fridays) | Ongoing, start immediately | ðŸ”´ High |
| 2 | **Interview scripts** â€” Finalize 15-min pre-screen + longer engineering interview script. Review Fred's edited version and iterate. | End of this week (Jan 31) / first thing next week at latest | ðŸ”´ High |
| 3 | **Bucket & present** â€” Create a clean 3-area summary: (a) list of all automation fixes done, (b) dashboard with volume + quality overlay, (c) system improvements roadmap | Next check-in | ðŸŸ¡ Medium |
| 4 | **Hackathon prep doc** â€” List of topics (engineering + internal process), toolkit to bring, comparison with engineering's approach | Before hackathon (3 weeks) | ðŸŸ¡ Medium |
| 5 | **AI unification doc** â€” Share the "Phased approach to unify AI developments" with the team, cross-reference with Ehor/Sergei's tooling | This/next week | ðŸŸ¡ Medium |
| 6 | **Notification bot** â€” Reconfigure the Slack/notification bot (went haywire, was cancelled). Reduce noise per Val's feedback. | Next week | ðŸŸ¡ Medium |
| 7 | **Dashboard enhancements** â€” Add quality overlay, rock star rate tracking, overdue candidate alerts (2-day SLA) | Ongoing | ðŸŸ¡ Medium |
| 8 | **Source attribution dashboard** â€” Final pass, then share with team | This week | ðŸŸ¡ Medium |
| 9 | **Pipeline guides** â€” Document how the automation pipeline works so others can adapt/extend | Before hackathon | ðŸŸ¢ Normal |
| 10 | **Salo accommodation** â€” Sort out lodging for hackathon (7 bedrooms, might need to bunk or find own) | Before hackathon | ðŸŸ¢ Normal |

### 3. Fred's Feedback & Expectations

**What Fred liked:**
- AI scoring pipeline with MAE tracking âœ…
- Source attribution dashboard âœ…
- Gemini Flash 3 migration (free, better structured) âœ…
- Self-hosted, Zapier-free architecture âœ…
- The automation inventory doc âœ…

**What Fred wants more of:**
- **Proactive communication** â€” manage up, don't wait for Friday
- **Incremental deliverables** â€” definition of done = someone starts using it and gives feedback
- **Team benefit framing** â€” every personal automation project must answer "how does this benefit the team?"
- **Collaboration over solo work** â€” *"True collaboration is where the magic happens. You can only go so far working alone."*
- **Standardization** â€” common evals, shared models, shared AI accounts across Team Charlie
- **Front-load the hard stuff** â€” prioritize hardest parts first in any deliverable

**On Patrick/Rahul/Brent:**
Fred is frustrated with their execution. Doesn't trust Patrick/Rahul to refine outbound targeting. Sees Brent struggling with AI tools. Wants Mart to help elevate the team.

### 4. Hackathon Prep Notes

**Location:** Salo (7 bedrooms available)
**Timeline:** ~3 weeks from now
**Mart's prep checklist:**
- [ ] Prepare list of hackathon topics split by: engineering topics vs internal process topics
- [ ] Document "intelligence for engineering layer" vs "intelligence for business layer"
- [ ] Prepare toolkit presentation: what tools, how they work, how AI is used for design/eval/test
- [ ] Compare with engineering team's toolkit (Cursor, other IDEs, models used)
- [ ] Prepare demo stacks: small, modular pieces that can be worked on in pairing sessions
- [ ] Think about: How does the publishing system connect to business intelligence layer?
- [ ] Think about: Standard evals, tests, and alerts system everyone can plug into
- [ ] Think about: Security for agent-to-agent credential sharing
- [ ] Think about: Cost control â€” which AI tools are sufficient vs. redundant?
- [ ] Local models setup: leverage Mac Minis + open source models (DeepSeek, Qwen etc.)

**Fred's hackathon vision:**
Everyone in a room, each working on automating their own process, sharing LLM orchestration patterns, putting evals around everything, paired programming. Like the Cursor team story â€” *"Four 20-year-olds, no ego, everyone equal, helping each other break through."*

---

## Action Items Executed (Jan 30 evening)

### Task 1: Dashboard Quality Overlay + Rock Star Rate âœ…
- **Branch:** `feat/quality-overlay-rockstar-rate` pushed to `cere-io/HR-2026-E2E`
- **PR:** https://github.com/cere-io/HR-2026-E2E/pull/new/feat/quality-overlay-rockstar-rate
- **What was added:**
  - New `quality-metrics.ts` service â€” fetches all scored candidates, computes rock star rate (â‰¥8), source attribution, daily snapshots
  - Quality hero stats: total rock stars, avg AI/human scores, std dev, top 10% threshold (Î¼+2Ïƒ)
  - Inbound vs Outbound comparison cards
  - 14-day daily volume + quality chart with rock star overlay
  - Source attribution table with per-source rock star rate
  - New `/v3/api/quality` JSON endpoint
- **Auto-deploys** to Railway once merged to main

### Task 2: Daily Execution Notes âœ…
- **Google Doc:** https://docs.google.com/document/d/1G88aT5a341H9-srkQhlL5UnhrWKzc4_QIlVV2Iap4RQ/edit
- Template: Date, Done, Learned, Blocked, Next, Metrics
- First entry: Jan 30 covering all today's work
- Ready to share in #cere-hiring-internal

### Task 3: Hackathon Prep Doc âœ…
- **Google Doc:** https://docs.google.com/document/d/1GVwNvELveqtoc_-fbWFSK_jSTXKCAdCFy7FmwRO435k/edit
- Sections: Topics (5 areas), Toolkit (3 categories), Engineering Comparison, Best Practices, Demo Stacks (6 demos), Local Models (Mac Minis + open source)
- Ready to share with team
